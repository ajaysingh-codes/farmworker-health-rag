{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ADTRu2VUFzc_"
      ],
      "authorship_tag": "ABX9TyMZ8c7oRRVjwzY3kdx9/L8E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaysingh-codes/farmworker-health-rag/blob/main/farmworker_health_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Farmworker Health Research RAG System\n",
        "\n",
        "**A smart search system for scientific literature on farmworker health, chemical exposures, and occupational stressors**\n",
        "\n",
        "Built with BM25 + Semantic Search | Interactive comparison interface | Optimized for health research papers\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QWo30qxeFUHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Installing Required Libraries\n",
        "This cell installs all the necessary Python packages for our RAG system:\n",
        "- `sentence-transformers`: For creating semantic embeddings of text\n",
        "- `bm25s`: For keyword-based search (BM25 algorithm)\n",
        "- `pypdf2`: For extracting text from PDF files\n",
        "- `pandas`: For data manipulation\n",
        "- `numpy`: For numerical operations\n",
        "- `joblib`: For saving/loading embeddings\n",
        "- `ipywidgets`: For creating the interactive interface"
      ],
      "metadata": {
        "id": "ADTRu2VUFzc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers bm25s pypdf2 pandas numpy joblib ipywidgets -q\n",
        "print(\"Libraries installed successfully\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SMbo3Du8FW-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries and Creating Workspace\n",
        "This cell:\n",
        "1. Imports all the libraries we'll use throughout the project\n",
        "2. Creates a dedicated folder `/content/papers` for your PDF files\n",
        "3. Sets up the basic environment for our RAG system"
      ],
      "metadata": {
        "id": "0adljKo_H2ZO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKwRGdlH4XBF"
      },
      "outputs": [],
      "source": [
        "# Import all necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import bm25s\n",
        "import joblib\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from IPython.display import display, Markdown\n",
        "import ipywidgets as widgets\n",
        "from PyPDF2 import PdfReader\n",
        "from datetime import datetime\n",
        "\n",
        "# Create a folder for your PDFs\n",
        "pdf_folder = \"/content/papers\"\n",
        "if not os.path.exists(pdf_folder):\n",
        "    os.makedirs(pdf_folder)\n",
        "    print(f\"‚úÖ Created folder: {pdf_folder}\")\n",
        "else:\n",
        "    print(f\"üìÅ Folder already exists: {pdf_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Your Research Papers\n",
        "Upload your 5 PDF papers about farmworker health:\n",
        "- Click 'Choose Files' to select your PDFs\n",
        "- Papers will be moved to the `papers` folder\n",
        "- You'll see confirmation for each uploaded file"
      ],
      "metadata": {
        "id": "LyjiJSd2JV8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Click 'Choose Files' below to upload your 5 PDFs:\\n\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to the papers folder\n",
        "for filename in uploaded.keys():\n",
        "    destination = os.path.join(pdf_folder, filename)\n",
        "    os.rename(filename, destination)\n",
        "    print(f\"‚úÖ Uploaded: {filename}\")\n",
        "\n",
        "# Verify and list all PDFs in the folder\n",
        "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
        "print(f\"üìä Total PDFs in folder: {len(pdf_files)}\")\n",
        "for i, pdf in enumerate(pdf_files, 1):\n",
        "    print(f\"  {i}. {pdf}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7hlh6-xJJVg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking Function with Overlap\n",
        "Using 150-word chunks with 20% overlap for optimal context"
      ],
      "metadata": {
        "id": "IcndPkDoVWeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import re\n",
        "\n",
        "def get_chunks_fixed_size_with_overlap(text: str, chunk_size: int, overlap_fraction: float) -> List[str]:\n",
        "  \"\"\"\n",
        "  Splits text into fixed-size chunks with overlap.\n",
        "\n",
        "  Parameters:\n",
        "  - text (str): The text to be split into chunks.\n",
        "  - chunk_size (int): The desired size of each chunk.\n",
        "  - overlap_fraction (float): The fraction of overlap between chunks (0.2 = 20% overlap)\n",
        "\n",
        "  Returns:\n",
        "  - List[str]: A list of text chunks where each chunk might overlap with its adjacent chunk.\n",
        "\n",
        "  \"\"\"\n",
        "  # Split text into individual words\n",
        "  text_words = text.split()\n",
        "\n",
        "  # Calculate the number of words to overlap\n",
        "  overlap_int = int(chunk_size * overlap_fraction)\n",
        "\n",
        "  #Initialize a list to store resulting chunks\n",
        "  chunks = []\n",
        "\n",
        "  # Create chunks with overlap\n",
        "  for i in range(0, len(text_words), chunk_size):\n",
        "    # Include overlap from previous chunk\n",
        "    chunk_words = text_words[max(i - overlap_int, 0): i + chunk_size]\n",
        "\n",
        "    # Join words to form chunk\n",
        "    chunk = \" \".join(chunk_words)\n",
        "\n",
        "    chunks.append(chunk)\n",
        "\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "hgm6oXGZVWR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Extract and Process PDFs\n",
        "Extract text and create chunks from each paper"
      ],
      "metadata": {
        "id": "aOLwWWeDaYjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_chunk_pdf(pdf_path, chunk_size=150, overlap=0.2):\n",
        "    \"\"\"Extract text from PDF and return chunks\"\"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        full_text = \"\"\n",
        "\n",
        "        # Extract all pages\n",
        "        for page in reader.pages:\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                full_text += text + \" \"\n",
        "\n",
        "        # Clean text\n",
        "        full_text = re.sub(r'\\s+', ' ', full_text)\n",
        "        full_text = re.sub(r'\\n+', ' ', full_text)\n",
        "\n",
        "        # Create chunks\n",
        "        chunks = get_chunks_fixed_size_with_overlap(full_text, chunk_size, overlap)\n",
        "\n",
        "        return chunks, len(reader.pages), True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return [], 0, False\n",
        "\n",
        "# Process all PDFs and create dataset\n",
        "PAPERS_DATA = []\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
        "    paper_name = pdf_file.replace('.pdf', '')\n",
        "\n",
        "    # Extract and chunk\n",
        "    chunks, num_pages, success = extract_and_chunk_pdf(pdf_path)\n",
        "\n",
        "    if success:\n",
        "        for i, chunk in enumerate(chunks):\n",
        "\n",
        "            PAPERS_DATA.append({\n",
        "                'content': chunk, # Main text for search\n",
        "                'source': paper_name, # Which paper it's from\n",
        "                'id': f\"{paper_name}_{i}\" # Pos in paper\n",
        "            })\n",
        "    else:\n",
        "        print(f\"  ‚ùå Failed to process\")\n",
        "\n",
        "print(f\"\\n‚úÖ Total chunks created: {len(PAPERS_DATA)}\")"
      ],
      "metadata": {
        "id": "uydg2JLDI3K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Implement Retrieval functions\n",
        "Build BM25 (keyword) and Semantic Search capabilities."
      ],
      "metadata": {
        "id": "SW7Y1jyAeM3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create corpus for searching (combining all text for each chunk)\n",
        "corpus = [x['content'] for x in PAPERS_DATA]\n",
        "\n",
        "print(f\"Corpus ready with {len(corpus)} chunks\")\n",
        "print(f\"From {len(set([x['source'] for x in PAPERS_DATA]))} papers\")"
      ],
      "metadata": {
        "id": "Og3OHquYdMaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3.1: BM25 (Keyword) Search\n"
      ],
      "metadata": {
        "id": "PYFDQEDsgWGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create BM25 Retriever\n",
        "BM25_RETRIEVER = bm25s.BM25(corpus=corpus)\n",
        "\n",
        "# Tokenize the corpus\n",
        "TOKENIZED_DATA = bm25s.tokenize(corpus)\n",
        "\n",
        "# Index the tokenized data\n",
        "BM25_RETRIEVER.index(TOKENIZED_DATA)"
      ],
      "metadata": {
        "id": "FAn4vRbKfZZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bm25_retrieve(query: str, top_k: int = 5):\n",
        "  \"\"\"\n",
        "  BM25 retrieval - keyword-based search\n",
        "  \"\"\"\n",
        "\n",
        "  tokenized_query = bm25s.tokenize(query)\n",
        "\n",
        "  # Retrieve documents\n",
        "  results, scores = BM25_RETRIEVER.retrieve(tokenized_query, k=top_k)\n",
        "\n",
        "  # Get indices\n",
        "  results = results[0]\n",
        "  top_k_indices = [corpus.index(result) for result in results]\n",
        "\n",
        "  return top_k_indices\n",
        "\n",
        "# Test BM25\n",
        "test_query = \"pesticide exposure health effects\"\n",
        "bm25_results = bm25_retrieve(test_query, top_k=3)\n",
        "\n",
        "print(f\"Query: {test_query}\")\n",
        "for idx in bm25_results[:3]:\n",
        "  print(f\"\\n Source: {PAPERS_DATA[idx]['source']}\")\n",
        "  print(f\"Preview: {PAPERS_DATA[idx]['content'][:200]}...\")"
      ],
      "metadata": {
        "id": "KaVP8UDvg0Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3.2 : Semantic Search With Embeddings\n",
        "Create embeddings for semantic understanding"
      ],
      "metadata": {
        "id": "7L6kA1EWid9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Initialize the embedding model\n",
        "# Using a model suitable for scientific/health content\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for all chunks\n",
        "EMBEDDINGS = model.encode(corpus, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "J9snXWjjiOyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic Search Function\n",
        "def semantic_search_retrieve(query, top_k=5):\n",
        "  \"\"\"\n",
        "  Semantic search using embeddings\n",
        "  \"\"\"\n",
        "  query_embedding = model.encode(query)\n",
        "\n",
        "  # Calculate cosine similarity\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  similarities = cosine_similarity([query_embedding], EMBEDDINGS)[0]\n",
        "\n",
        "  # Get top-k indices\n",
        "  top_k_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "  return top_k_indices.tolist()\n",
        "\n",
        "# Test semantic search\n",
        "semantic_results = semantic_search_retrieve(test_query, top_k=3)\n",
        "\n",
        "print(f\"Query: {test_query}\")\n",
        "for idx in semantic_results[:3]:\n",
        "  print(f\"\\n Source: {PAPERS_DATA[idx]['source']}\")\n",
        "  print(f\"Preview: {PAPERS_DATA[idx]['content'][:200]}...\")"
      ],
      "metadata": {
        "id": "5bvxVMmpkZJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3.3: Reciprocal Rank Fusion (RRF)\n",
        "Combine BM25 and Semantic Search using RRF - to reward documents/papers that rank higher in each retrieval technique list."
      ],
      "metadata": {
        "id": "1iyWzifJmMb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RRF Example\n",
        "# RRF function (exactly from the example)\n",
        "def reciprocal_rank_fusion(list1, list2, top_k=5, K=60):\n",
        "    \"\"\"\n",
        "    Combine results from BM25 and Semantic Search\n",
        "    \"\"\"\n",
        "    rrf_scores = {}\n",
        "\n",
        "    # Calculate RRF scores\n",
        "    for lst in [list1, list2]:\n",
        "        for rank, item in enumerate(lst, start=1):\n",
        "            if item not in rrf_scores:\n",
        "                rrf_scores[item] = 0\n",
        "            rrf_scores[item] += 1 / (rank + K)\n",
        "\n",
        "    # Sort by score\n",
        "    sorted_items = sorted(rrf_scores, key=rrf_scores.get, reverse=True)\n",
        "\n",
        "    return sorted_items[:top_k]\n",
        "\n",
        "# Test RRF\n",
        "bm25_list = bm25_retrieve(test_query, top_k=5)\n",
        "semantic_list = semantic_search_retrieve(test_query, top_k=5)\n",
        "rrf_list = reciprocal_rank_fusion(bm25_list, semantic_list, top_k=5)\n",
        "\n",
        "print(\"üîÄ Reciprocal Rank Fusion Results:\")\n",
        "print(f\"BM25 returned: {bm25_list}\")\n",
        "print(f\"Semantic returned: {semantic_list}\")\n",
        "print(f\"RRF combined: {rrf_list}\")"
      ],
      "metadata": {
        "id": "64YCpQeGmjkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sV3nQ5SMm1tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_papers(indices):\n",
        "  \"\"\"\n",
        "  Retrieve paper chunks by indices\n",
        "  \"\"\"\n",
        "  return [PAPERS_DATA[index] for index in indices]\n",
        "\n",
        "retrieved_papers = query_papers(rrf_list[:3])\n",
        "for i, paper in enumerate(retrieved_papers, 1):\n",
        "  print(f\"\\n{i}. Source: {paper['source']}\")\n",
        "  print(f\"  Content: {paper['content'][:200]}...\")"
      ],
      "metadata": {
        "id": "2Yh7AZMNnDa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EQCGl7pMoZNf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}